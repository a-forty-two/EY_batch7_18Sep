{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/EY_batch7_18Sep/blob/main/18_Sep_005_llm_intro_tokenizer%20and%20simple%20LLM%20pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khj7ylHb5Y4K"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQlux2v15Y4L"
      },
      "source": [
        "## 1.2. Pulling In Our First LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3DfWbMaOQxe"
      },
      "source": [
        "Instead of constructing things from the ground up, this course will focus on spot-lighting tools that you can use and diving into them as necessary to figure out exactly how they work. And the best tool to start our journey into language modeling is **HuggingFace &#x1F917;!**\n",
        "\n",
        "[**HuggingFace**](https://huggingface.co/) is an open-source community that offers simple strategies for accessing, uploading, and using large deep learning models for testing and deployment. The topics they support span many tasks and modalities, but we'll be focusing on large language models (**LLMs**) for most of this course.\n",
        "\n",
        "When searching through the [HuggingFace Models catalog](https://huggingface.co/models?sort=downloads&search=bert), you'll quickly stumble upon the [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) model. Taking a look at its card, you'll see several interesting things:\n",
        "\n",
        "1. Loading in the model requires the use of the [`transformers`](https://github.com/huggingface/transformers) package. This is the HuggingFace package used to support most of the platform's language modeling code. Its name, `transformers`, refers to the primary architectural structure underlying many of these models, and we'll be talking about this structure in some detail throughout the next notebook. From here on out, you'll want to get comfortable with `transformers` and will be using it quite a bit, so feel free to search around and dive into the source code if you feel like it!\n",
        "2. The card describes a default version that can be pulled in for mask filling (to be discussed) via its [Pipelines]([https://huggingface.co/docs/transformers/main_classes/pipelines]) support. By **pipeline**, we mean the end-to-end process of going from a human-reasonable input to a human-reasonable output. This makes it super-easy to pull in the model and helps you to forget that there is a tensor-in/tensor-out differentiable process going on somewhere under the hood.\n",
        "\n",
        "As a representative example, we can go ahead and pull in the discussed [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) model and test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ERP_WrTRP2N",
        "outputId": "78efe416-eb12-45a0-b0cb-512b98ad76a8",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.23936112225055695,\n",
              "  'token': 4827,\n",
              "  'token_str': 'fashion',\n",
              "  'sequence': \"hello i'm a graceful fashion model.\"},\n",
              " {'score': 0.2011405974626541,\n",
              "  'token': 2535,\n",
              "  'token_str': 'role',\n",
              "  'sequence': \"hello i'm a graceful role model.\"},\n",
              " {'score': 0.048176608979701996,\n",
              "  'token': 2210,\n",
              "  'token_str': 'little',\n",
              "  'sequence': \"hello i'm a graceful little model.\"},\n",
              " {'score': 0.028926273807883263,\n",
              "  'token': 9271,\n",
              "  'token_str': 'runway',\n",
              "  'sequence': \"hello i'm a graceful runway model.\"},\n",
              " {'score': 0.020872579887509346,\n",
              "  'token': 2047,\n",
              "  'token_str': 'new',\n",
              "  'sequence': \"hello i'm a graceful new model.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "## Loading in the pipeline and predict the mask fill (example from model card)\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "unmasker(\"Hello I'm a graceful [MASK] model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfUHbutJO4Nr"
      },
      "source": [
        "**Amazing! It just works!** Under the hood, there's a deep learning model somewhere - crunching numbers and spitting out probabilities to make all of this happen - but it's easy to forget that sometimes. It's especially easy to forget when the model you're dealing with is actually generating human-sounding text, at which point you may start to wonder if it's connected to a human brain somewhere in a warehouse in California. But that's what this course is for: **to see what's actually going on behind the scenes and know how to use it to make good systems**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw4faamM5Y4N"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA9vUG_Ny9Cn"
      },
      "source": [
        "## 1.3. Dissecting The Pipeline\n",
        "\n",
        "Looking at this resolution - where we just see the pipeline taking strings in and spitting a dictionary out - isn't really helping our understanding much, so let's see what's actually going on with the pipeline. We can peel back the layer of abstraction just a little to see the structure inside of the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atoVpxD7zgFO",
        "outputId": "a5792920-b2e5-4376-9ba4-17a017914996",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "preprocess outputs:\n",
            " {'input_ids': tensor([[  101,  7592,  1010,  2720,  1012, 14324,   999,  2129,  2003,  2009,\n",
            "           103,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} \n",
            "\n",
            "forward outputs:\n",
            " ModelOutput([('logits', tensor([[[ -6.6978,  -6.6436,  -6.6542,  ...,  -5.9664,  -5.8454,  -4.0651],\n",
            "         [ -7.1922,  -7.0817,  -7.2808,  ...,  -6.6275,  -6.7937,  -5.0987],\n",
            "         [-14.1895, -14.3331, -14.3951,  ..., -11.4550, -10.5485, -10.7081],\n",
            "         ...,\n",
            "         [ -7.8704,  -7.9647,  -7.8207,  ...,  -6.7212,  -7.0204,  -4.0526],\n",
            "         [-10.7055, -10.2556, -10.7905,  ...,  -9.1913, -10.1996,  -0.9692],\n",
            "         [-10.9574, -11.0610, -10.8301,  ...,  -9.3834,  -9.7457,  -8.4909]]])), ('input_ids', tensor([[  101,  7592,  1010,  2720,  1012, 14324,   999,  2129,  2003,  2009,\n",
            "           103,  1029,   102]]))]) \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.23866014182567596,\n",
              "  'token': 2183,\n",
              "  'token_str': 'going',\n",
              "  'sequence': 'hello, mr. bert! how is it going?'},\n",
              " {'score': 0.07178756594657898,\n",
              "  'token': 2017,\n",
              "  'token_str': 'you',\n",
              "  'sequence': 'hello, mr. bert! how is it you?'},\n",
              " {'score': 0.05827958881855011,\n",
              "  'token': 6230,\n",
              "  'token_str': 'happening',\n",
              "  'sequence': 'hello, mr. bert! how is it happening?'},\n",
              " {'score': 0.056334324181079865,\n",
              "  'token': 2651,\n",
              "  'token_str': 'today',\n",
              "  'sequence': 'hello, mr. bert! how is it today?'},\n",
              " {'score': 0.052870072424411774,\n",
              "  'token': 2085,\n",
              "  'token_str': 'now',\n",
              "  'sequence': 'hello, mr. bert! how is it now?'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, BertTokenizer, BertModel, FillMaskPipeline, AutoModelForMaskedLM, BertForMaskedLM, BertForPreTraining\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel        ## General-purpose fully-automatic\n",
        "from transformers import AutoModelForMaskedLM            ## Default import for FillMaskPipeline\n",
        "from transformers import BertTokenizer, BertForMaskedLM  ## Realized components after automatic resolution\n",
        "\n",
        "class MyMlmPipeline(FillMaskPipeline):\n",
        "    def __init__(self):\n",
        "        ## The fully-automatic version\n",
        "        super().__init__(\n",
        "            tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
        "            model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "        )\n",
        "\n",
        "    def __call__(self, string, verbose=False):\n",
        "        ## Verbose argument just there for our convenience\n",
        "        input_tensors = self.preprocess(string)\n",
        "        if verbose: print('\\npreprocess outputs:\\n', input_tensors, '\\n')\n",
        "        output_tensors = self.forward(input_tensors)\n",
        "        if verbose: print('forward outputs:\\n', output_tensors, '\\n')\n",
        "        output = self.postprocess(output_tensors)\n",
        "        return output\n",
        "\n",
        "    # def preprocess(self, string):\n",
        "    #     string = [string] if isinstance(string, str) else string\n",
        "    #     inputs = self.tokenizer(string, return_tensors=\"pt\")\n",
        "    #     return inputs\n",
        "\n",
        "    # def forward(self, tensor_dict):\n",
        "    #     output_tensors = self.model.forward(**tensor_dict)\n",
        "    #     return {**output_tensors, **tensor_dict}\n",
        "\n",
        "    # def postprocess(self, tensor_dict):\n",
        "    #     ## Very Task-specific; see FillMaskPipeline.postprocess\n",
        "    #     return super().postprocess(tensor_dict)\n",
        "\n",
        "unmasker = MyMlmPipeline()\n",
        "unmasker(\"Hello, Mr. Bert! How is it [MASK]?\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLoEmptNztUb"
      },
      "source": [
        "We can also see that the model is largely comprised of two main components:\n",
        "- `tokenizer`: The strategy to convert the input strings to something usable by the model.\n",
        "- `model`: The deep learning model responsible for the input-tensor-to-output-tensor conversion.\n",
        "\n",
        "With these, the pipeline is able to support its streamlined interface with a pretty intuitive organization scheme:  \n",
        "- `preprocess`: human-intuitive input $\\to$ tensor inputs. Facilitated by `tokenizer`\n",
        "- `forward`: tensor inputs $\\to$ tensor outputs. Facilitated by `model`\n",
        "- `postprocess`: tensor outputs $\\to$ human-intuitive outputs. Facilitated by the pipeline task.\n",
        "\n",
        "For deep learning, this actually seems pretty reasonable; the model reasons in numbers, and you probably don't want to expose that to the typical user when your domain is language. This makes it very easy for a typical starting user to just pick up the models and roll with them, so hopefully you feel a bit more comfortable when approaching the open-sourced LLM ecosystem!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQtJ2jHc5Y4N"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttughVjW5Y4O",
        "outputId": "6dd44c37-2841-4518-97f8-6d2f47199f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================================\n",
            "GPU SPECIFICATION\n",
            "===================================================\n",
            "\n",
            "\n",
            "===================================================\n",
            "MEMORY SPECIFICATION\n",
            "===================================================\n",
            "\n",
            "MemTotal:       13290460 kB\n",
            "MemFree:         4665140 kB\n",
            "MemAvailable:   10846260 kB\n",
            "Buffers:          399764 kB\n",
            "Cached:          5897256 kB\n",
            "SwapCached:            0 kB\n",
            "Active:          1170936 kB\n",
            "Inactive:        7078212 kB\n",
            "Active(anon):       1388 kB\n",
            "Inactive(anon):  1952616 kB\n",
            "Active(file):    1169548 kB\n",
            "Inactive(file):  5125596 kB\n",
            "Unevictable:           8 kB\n",
            "Mlocked:               8 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               300 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:       1952320 kB\n",
            "Mapped:          1237192 kB\n",
            "Shmem:              1736 kB\n",
            "KReclaimable:     211380 kB\n",
            "Slab:             259880 kB\n",
            "SReclaimable:     211380 kB\n",
            "SUnreclaim:        48500 kB\n",
            "KernelStack:        6080 kB\n",
            "PageTables:        36028 kB\n",
            "SecPageTables:         0 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6645228 kB\n",
            "Committed_AS:    4466300 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       12416 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1144 kB\n",
            "HardwareCorrupted:     0 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "Unaccepted:            0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:       76600 kB\n",
            "DirectMap2M:     5163008 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bash: line 6: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "echo \"\"\"\n",
        "===================================================\n",
        "GPU SPECIFICATION\n",
        "===================================================\n",
        "\"\"\"\n",
        "nvidia-smi\n",
        "echo \"\"\"\n",
        "===================================================\n",
        "MEMORY SPECIFICATION\n",
        "===================================================\n",
        "\"\"\"\n",
        "cat /proc/meminfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuoAMMjq5Y4O"
      },
      "source": [
        "**So yeah, decent compute budget, *but not infinite*!**\n",
        "\n",
        "Before starting the next notebook, please restart the jupyter kernel by running the code cell below. This will prevent memory issues in future notebooks and will keep the instance memory load from overpowering our compute budget."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzRxZZUZ5Y4O"
      },
      "source": [
        "-------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}